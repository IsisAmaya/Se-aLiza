{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9581491,"sourceType":"datasetVersion","datasetId":5842422}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install 'git+https://github.com/facebookresearch/fvcore'\n!pip install simplejson\n!pip install einops\n!pip install timm\n!pip install psutil\n!pip install scikit-learn\n!pip install opencv-python\n!pip install tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:56:31.542895Z","iopub.execute_input":"2024-10-15T03:56:31.543185Z","iopub.status.idle":"2024-10-15T03:58:12.076464Z","shell.execute_reply.started":"2024-10-15T03:56:31.543153Z","shell.execute_reply":"2024-10-15T03:58:12.075410Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/facebookresearch/fvcore\n  Cloning https://github.com/facebookresearch/fvcore to /tmp/pip-req-build-61mk65bb\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/fvcore /tmp/pip-req-build-61mk65bb\n  Resolved https://github.com/facebookresearch/fvcore to commit b25ff8c84ebb2fe88b61b7a8994b9571a1e13bab\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore==0.1.6) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore==0.1.6)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore==0.1.6) (6.0.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore==0.1.6) (4.66.4)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from fvcore==0.1.6) (2.4.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore==0.1.6) (10.3.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore==0.1.6) (0.9.0)\nCollecting iopath>=0.1.7 (from fvcore==0.1.6)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore==0.1.6) (4.12.2)\nCollecting portalocker (from iopath>=0.1.7->fvcore==0.1.6)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nBuilding wheels for collected packages: fvcore, iopath\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.6-py3-none-any.whl size=65574 sha256=c50655527ac7afd65898e7e18abcb4b0259bfe898cdda5134cc352b393905606\n  Stored in directory: /tmp/pip-ephem-wheel-cache-0pbe9076/wheels/a6/fe/66/6e6bfe16b94f13bf3e150e48ad58e6dc20de14125d0550621e\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=a71e050dad4b82fbd39ac672746149d03d4f7d4cf0ebf757e351b9c23616f420\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built fvcore iopath\nInstalling collected packages: yacs, portalocker, iopath, fvcore\nSuccessfully installed fvcore-0.1.6 iopath-0.1.10 portalocker-2.10.1 yacs-0.1.8\nCollecting simplejson\n  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nDownloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: simplejson\nSuccessfully installed simplejson-3.19.3\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (1.0.9)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.19.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.25.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (10.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->timm) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (5.9.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.10.0.84)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.16.2)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.62.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.6)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (70.0.0)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers --upgrade","metadata":{"execution":{"iopub.status.busy":"2024-10-15T03:58:12.078979Z","iopub.execute_input":"2024-10-15T03:58:12.079772Z","iopub.status.idle":"2024-10-15T03:58:35.188461Z","shell.execute_reply.started":"2024-10-15T03:58:12.079721Z","shell.execute_reply":"2024-10-15T03:58:35.187283Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers\n  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed transformers-4.45.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset\n\nclass VideoDataset(Dataset):\n    def __init__(self, root_dir, transform=None, max_frames=None, frame_skip=1):\n        \"\"\"\n        Dataset para cargar videos desde carpetas categorizadas por etiquetas.\n        \n        Args:\n            root_dir (str): Directorio raíz que contiene las carpetas con los videos.\n            transform (callable, optional): Transformaciones que se aplicarán a cada frame.\n            max_frames (int, optional): Máximo de frames a extraer por video.\n            frame_skip (int, optional): Número de frames a saltar entre extracciones.\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n        self.max_frames = max_frames\n        self.frame_skip = frame_skip\n        self.video_paths, self.labels = self._load_videos_and_labels(root_dir)\n        self.label_map = {label: idx for idx, label in enumerate(sorted(set(self.labels)))}\n\n    def _load_videos_and_labels(self, root_dir):\n        video_paths = []\n        labels = []\n        # Recorre cada subdirectorio (etiqueta) dentro del directorio raíz\n        for label in os.listdir(root_dir):\n            label_dir = os.path.join(root_dir, label)\n            if os.path.isdir(label_dir):\n                # Recorre todos los videos en el subdirectorio\n                for video in os.listdir(label_dir):\n                    if video.endswith(('.MOV', '.mov', '.mp4')):  # Ajusta según los formatos de video\n                        video_paths.append(os.path.join(label_dir, video))\n                        labels.append(label)\n        return video_paths, labels\n\n    def __len__(self):\n        return len(self.video_paths)\n\n    def extract_frames_from_video(self, video_path):\n        frames = []\n        cap = cv2.VideoCapture(video_path)\n        frame_count = 0\n\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            if frame_count % self.frame_skip == 0:\n                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                if self.transform:\n                    frame_rgb = self.transform(frame_rgb)\n                frames.append(frame_rgb)\n\n            frame_count += 1\n            if self.max_frames and len(frames) >= self.max_frames:\n                break\n\n        cap.release()\n\n        # Verificar el número mínimo de frames y aplicar padding si es necesario\n        min_frames = 32\n        while len(frames) < min_frames:\n            padding_frame = torch.zeros_like(frames[0])\n            frames.append(padding_frame)\n\n        return torch.stack(frames)\n\n\n    def __getitem__(self, idx):\n        video_path = self.video_paths[idx]\n        frames = self.extract_frames_from_video(video_path)\n        label = self.label_map[self.labels[idx]] # Convertir la etiqueta a índice numérico\n        # Depurar las dimensiones\n        #print(f\"Video {idx}: {frames.shape}, Label: {label}\")\n        return frames, label\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T04:56:04.813610Z","iopub.execute_input":"2024-10-15T04:56:04.814364Z","iopub.status.idle":"2024-10-15T04:56:04.829162Z","shell.execute_reply.started":"2024-10-15T04:56:04.814324Z","shell.execute_reply":"2024-10-15T04:56:04.828239Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def pad_collate_fn(batch):\n    \"\"\"\n    Asegura que todos los videos en un lote tengan el mismo número de frames \n    mediante padding (relleno con ceros).\n    \"\"\"\n    videos, labels = zip(*batch)\n\n    # Encontrar el número máximo de frames en el lote\n    max_frames = max(video.size(0) for video in videos)\n\n    # Rellenar los videos con frames de ceros para que tengan la misma longitud\n    padded_videos = []\n    for video in videos:\n        padding = torch.zeros((max_frames - video.size(0), *video.size()[1:]))\n        padded_video = torch.cat((video, padding), dim=0)\n        padded_videos.append(padded_video)\n\n    # Apilar los videos y convertir las etiquetas en un tensor\n    batch_videos = torch.stack(padded_videos)\n    batch_labels = torch.tensor(labels)\n\n    return batch_videos, batch_labels\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T04:56:08.230292Z","iopub.execute_input":"2024-10-15T04:56:08.231140Z","iopub.status.idle":"2024-10-15T04:56:08.237709Z","shell.execute_reply.started":"2024-10-15T04:56:08.231098Z","shell.execute_reply":"2024-10-15T04:56:08.236754Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\nfrom torch.utils.data import DataLoader, random_split\n\n# Definir las transformaciones que se aplicarán a cada frame (resize, normalización, etc.)\ntransform = transforms.Compose([\n    transforms.ToPILImage(),  # Convertir cada frame a formato PIL para aplicar transformaciones\n    transforms.Resize((224, 224)),  # Redimensionar los frames a 224x224 píxeles (requerido por TimeSformer)\n    #transforms.RandomVerticalFlip(),\n    transforms.RandomHorizontalFlip(),\n    #transforms.AugMix(),\n    transforms.ToTensor(),  # Convertir a tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizar\n])\n\n#image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n\n# Crear el dataset\ndataset = VideoDataset(root_dir='/kaggle/input/words-lsc/Words', transform=transform, max_frames=32, frame_skip=2)\n\n# Porcentaje de datos que usarás para entrenamiento (ej. 80% para entrenamiento, 20% para validación)\ntrain_size = int(0.70 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Dividir el dataset en entrenamiento y validación\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Crear DataLoaders para los conjuntos de entrenamiento y validación\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=pad_collate_fn)\nval_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=pad_collate_fn)\n\nprint(f\"Tamaño del DataLoader de entrenamiento: {len(train_dataloader)}, Tamaño del DataLoader de validación: {len(val_dataloader)}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T05:00:39.280193Z","iopub.execute_input":"2024-10-15T05:00:39.281159Z","iopub.status.idle":"2024-10-15T05:00:39.298882Z","shell.execute_reply.started":"2024-10-15T05:00:39.281115Z","shell.execute_reply":"2024-10-15T05:00:39.297877Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Tamaño del DataLoader de entrenamiento: 50, Tamaño del DataLoader de validación: 22\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torchvision.transforms.functional import to_pil_image\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef visualize_sample_as_gif(dataloader, output_path='sample.gif'):\n    # Obtener un batch del dataloader\n    data_iter = iter(dataloader)\n    batch = next(data_iter)\n\n    # Extraer los frames y las etiquetas (asumiendo batch = (video_frames, labels))\n    video_frames, _ = batch  # video_frames shape: [batch_size, frames, channels, height, width]\n\n    # Selecciona un video del batch (ej. el primero)\n    frames = video_frames[0]  # frames shape: [frames, channels, height, width]\n\n    # Convertir cada frame a formato PIL\n    pil_frames = [to_pil_image(frame) for frame in frames]\n\n    # Guardar los frames como un GIF\n    pil_frames[0].save(\n        output_path, save_all=True, append_images=pil_frames[1:], \n        duration=100, loop=0\n    )\n\n    print(f\"GIF guardado en {output_path}\")\n\n# Uso del DataLoader para obtener una muestra (ejemplo con el val_dataloader)\nvisualize_sample_as_gif(val_dataloader, output_path='video_sample.gif')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T05:00:41.954169Z","iopub.execute_input":"2024-10-15T05:00:41.954826Z","iopub.status.idle":"2024-10-15T05:01:06.479902Z","shell.execute_reply.started":"2024-10-15T05:00:41.954786Z","shell.execute_reply":"2024-10-15T05:01:06.478931Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"GIF guardado en video_sample.gif\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoImageProcessor, AutoModelForVideoClassification, AdamW, TimesformerForVideoClassification, TimesformerConfig\nfrom transformers import VideoMAEConfig, VideoMAEModel, VideoMAEForVideoClassification\nfrom transformers import TimesformerConfig, TimesformerModel\n\n\nmodel = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\nmodel.config.num_labels = 3  # Cambiar el número de clases a 3\nmodel.classifier = torch.nn.Linear(model.config.hidden_size, 3)  # Reemplazar la capa de clasificación\n\n#configuration = VideoMAEConfig( image_size=224, patch_size=16, num_channels=3, num_frames=32, num_labels=3)\n\n#model = VideoMAEForVideoClassification(configuration)\n\n\n# Optimización solo de las últimas capas (ajuste fino)\nfor param in model.base_model.parameters():\n    param.requires_grad = False  # Congelamos las capas base\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T05:02:15.423340Z","iopub.execute_input":"2024-10-15T05:02:15.423936Z","iopub.status.idle":"2024-10-15T05:02:17.859402Z","shell.execute_reply.started":"2024-10-15T05:02:15.423890Z","shell.execute_reply":"2024-10-15T05:02:17.858587Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/22.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6df55d774e364facb4665e4ff22cc1eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/486M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c25e342c05b40ceb58e5e0e47edeeb6"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\ncriterion = torch.nn.CrossEntropyLoss()  # Función de pérdida para clasificación\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)  # Optimizador\n\nnum_epochs = 5  # Número de épocas\n\nfor epoch in range(num_epochs):\n    # ---- Entrenamiento ----\n    model.train()  # Ponemos el modelo en modo entrenamiento\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    for i, (frames, labels) in enumerate(train_dataloader):\n        frames = frames.to(device)  # Enviar los frames al dispositivo (GPU)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()  # Resetear gradientes\n\n        outputs = model(pixel_values=frames)  # Paso hacia adelante\n        loss = criterion(outputs.logits, labels)  # Calcular la pérdida\n\n        loss.backward()  # Retropropagación\n        optimizer.step()  # Actualizar los pesos\n\n        running_loss += loss.item()\n\n        # Cálculo de precisión en entrenamiento\n        _, predicted = torch.max(outputs.logits, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n\n    train_loss = running_loss / len(train_dataloader)\n    train_acc = 100 * correct_train / total_train\n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%')\n\n    # ---- Validación ----\n    model.eval()  # Ponemos el modelo en modo evaluación (sin cálculo de gradientes)\n    val_loss = 0.0\n    correct_val = 0\n    total_val = 0\n\n    with torch.no_grad():  # No calcular gradientes durante la validación\n        for frames, labels in val_dataloader:\n            frames = frames.to(device)\n            labels = labels.to(device)\n\n            outputs = model(pixel_values=frames)\n            loss = criterion(outputs.logits, labels)\n            val_loss += loss.item()\n\n            # Cálculo de precisión en validación\n            _, predicted = torch.max(outputs.logits, 1)\n            total_val += labels.size(0)\n            correct_val += (predicted == labels).sum().item()\n\n    val_loss = val_loss / len(val_dataloader)\n    val_acc = 100 * correct_val / total_val\n    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T05:02:28.316273Z","iopub.execute_input":"2024-10-15T05:02:28.316669Z","iopub.status.idle":"2024-10-15T05:21:10.097579Z","shell.execute_reply.started":"2024-10-15T05:02:28.316630Z","shell.execute_reply":"2024-10-15T05:21:10.096230Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch [1/5], Train Loss: 1.0144, Train Accuracy: 50.00%\nEpoch [1/5], Validation Loss: 0.8474, Validation Accuracy: 74.71%\nEpoch [2/5], Train Loss: 0.6658, Train Accuracy: 92.50%\nEpoch [2/5], Validation Loss: 0.5770, Validation Accuracy: 97.70%\nEpoch [3/5], Train Loss: 0.4577, Train Accuracy: 98.50%\nEpoch [3/5], Validation Loss: 0.4079, Validation Accuracy: 98.85%\nEpoch [4/5], Train Loss: 0.3340, Train Accuracy: 100.00%\nEpoch [4/5], Validation Loss: 0.3123, Validation Accuracy: 100.00%\nEpoch [5/5], Train Loss: 0.2522, Train Accuracy: 100.00%\nEpoch [5/5], Validation Loss: 0.2416, Validation Accuracy: 100.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Guardar los pesos del modelo entrenado\ntorch.save(model, 'senaliza-videos.pth')","metadata":{"execution":{"iopub.status.busy":"2024-10-15T04:52:27.971105Z","iopub.execute_input":"2024-10-15T04:52:27.971436Z","iopub.status.idle":"2024-10-15T04:52:28.456000Z","shell.execute_reply.started":"2024-10-15T04:52:27.971403Z","shell.execute_reply":"2024-10-15T04:52:28.455112Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\ntorch.cuda.memory_summary(device=None, abbreviated=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T04:52:28.458760Z","iopub.execute_input":"2024-10-15T04:52:28.459100Z","iopub.status.idle":"2024-10-15T04:52:28.481299Z","shell.execute_reply.started":"2024-10-15T04:52:28.459066Z","shell.execute_reply":"2024-10-15T04:52:28.480376Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      | 409971 KiB |    898 MiB |   8798 GiB |   8797 GiB |\\n|       from large pool | 409472 KiB |    898 MiB |   8718 GiB |   8718 GiB |\\n|       from small pool |    499 KiB |      1 MiB |     79 GiB |     79 GiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         | 409971 KiB |    898 MiB |   8798 GiB |   8797 GiB |\\n|       from large pool | 409472 KiB |    898 MiB |   8718 GiB |   8718 GiB |\\n|       from small pool |    499 KiB |      1 MiB |     79 GiB |     79 GiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      | 409967 KiB |    896 MiB |   8772 GiB |   8772 GiB |\\n|       from large pool | 409472 KiB |    896 MiB |   8693 GiB |   8692 GiB |\\n|       from small pool |    495 KiB |      1 MiB |     79 GiB |     79 GiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   | 466944 KiB |   1016 MiB |   1016 MiB | 573440 KiB |\\n|       from large pool | 464896 KiB |   1014 MiB |   1014 MiB | 573440 KiB |\\n|       from small pool |   2048 KiB |      2 MiB |      2 MiB |      0 KiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  56972 KiB | 232476 KiB |   2237 GiB |   2237 GiB |\\n|       from large pool |  55424 KiB | 230912 KiB |   2158 GiB |   2158 GiB |\\n|       from small pool |   1548 KiB |   2045 KiB |     79 GiB |     79 GiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     206    |     214    |  334276    |  334070    |\\n|       from large pool |      76    |      83    |  164235    |  164159    |\\n|       from small pool |     130    |     135    |  170041    |  169911    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     206    |     214    |  334276    |  334070    |\\n|       from large pool |      76    |      83    |  164235    |  164159    |\\n|       from small pool |     130    |     135    |  170041    |  169911    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      21    |      29    |      29    |       8    |\\n|       from large pool |      20    |      28    |      28    |       8    |\\n|       from small pool |       1    |       1    |       1    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      26    |      33    |  166913    |  166887    |\\n|       from large pool |      20    |      27    |  123329    |  123309    |\\n|       from small pool |       6    |       8    |   43584    |   43578    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"},"metadata":{}}]}]}